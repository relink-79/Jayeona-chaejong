{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69e32ff3-1a94-4daa-9ba2-19084f3e62de",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 환경셋팅\n",
    "Qlora를 위한 환경셋팅  \n",
    "클라우드에서 한다면 터미널에서 huggingface-cli login 으로 토큰 작성  \n",
    "이 버전 아니면 안됨!!! (무조건 오류남.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5a8463e-2fe5-4774-b5d1-14abd56977bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.51.3\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting peft==0.14.0\n",
      "  Downloading peft-0.14.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting trl==0.15.2\n",
      "  Downloading trl-0.15.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: filelock in /venv/main/lib/python3.12/site-packages (from transformers==4.51.3) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /venv/main/lib/python3.12/site-packages (from transformers==4.51.3) (0.30.2)\n",
      "Collecting numpy>=1.17 (from transformers==4.51.3)\n",
      "  Downloading numpy-2.3.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /venv/main/lib/python3.12/site-packages (from transformers==4.51.3) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /venv/main/lib/python3.12/site-packages (from transformers==4.51.3) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.51.3)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /venv/main/lib/python3.12/site-packages (from transformers==4.51.3) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers==4.51.3)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers==4.51.3)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /venv/main/lib/python3.12/site-packages (from transformers==4.51.3) (4.67.1)\n",
      "Requirement already satisfied: psutil in /venv/main/lib/python3.12/site-packages (from peft==0.14.0) (7.0.0)\n",
      "Collecting torch>=1.13.0 (from peft==0.14.0)\n",
      "  Downloading torch-2.7.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Collecting accelerate>=0.21.0 (from peft==0.14.0)\n",
      "  Downloading accelerate-1.7.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting datasets>=2.21.0 (from trl==0.15.2)\n",
      "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting rich (from trl==0.15.2)\n",
      "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets>=2.21.0->trl==0.15.2)\n",
      "  Downloading pyarrow-20.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.21.0->trl==0.15.2)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets>=2.21.0->trl==0.15.2)\n",
      "  Downloading pandas-2.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xxhash (from datasets>=2.21.0->trl==0.15.2)\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets>=2.21.0->trl==0.15.2)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.21.0->trl==0.15.2)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /venv/main/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.51.3) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /venv/main/lib/python3.12/site-packages (from requests->transformers==4.51.3) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /venv/main/lib/python3.12/site-packages (from requests->transformers==4.51.3) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /venv/main/lib/python3.12/site-packages (from requests->transformers==4.51.3) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /venv/main/lib/python3.12/site-packages (from requests->transformers==4.51.3) (2025.4.26)\n",
      "Collecting setuptools (from torch>=1.13.0->peft==0.14.0)\n",
      "  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting sympy>=1.13.3 (from torch>=1.13.0->peft==0.14.0)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch>=1.13.0->peft==0.14.0)\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch>=1.13.0->peft==0.14.0)\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch>=1.13.0->peft==0.14.0)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch>=1.13.0->peft==0.14.0)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch>=1.13.0->peft==0.14.0)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch>=1.13.0->peft==0.14.0)\n",
      "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch>=1.13.0->peft==0.14.0)\n",
      "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch>=1.13.0->peft==0.14.0)\n",
      "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.7.77 (from torch>=1.13.0->peft==0.14.0)\n",
      "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch>=1.13.0->peft==0.14.0)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch>=1.13.0->peft==0.14.0)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch>=1.13.0->peft==0.14.0)\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.26.2 (from torch>=1.13.0->peft==0.14.0)\n",
      "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.6.77 (from torch>=1.13.0->peft==0.14.0)\n",
      "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch>=1.13.0->peft==0.14.0)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch>=1.13.0->peft==0.14.0)\n",
      "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.3.1 (from torch>=1.13.0->peft==0.14.0)\n",
      "  Downloading triton-3.3.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->trl==0.15.2)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /venv/main/lib/python3.12/site-packages (from rich->trl==0.15.2) (2.19.1)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.21.0->trl==0.15.2)\n",
      "  Downloading aiohttp-3.12.12-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->trl==0.15.2)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.13.0->peft==0.14.0)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.13.0->peft==0.14.0)\n",
      "  Downloading MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /venv/main/lib/python3.12/site-packages (from pandas->datasets>=2.21.0->trl==0.15.2) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets>=2.21.0->trl==0.15.2)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets>=2.21.0->trl==0.15.2)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.21.0->trl==0.15.2)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.21.0->trl==0.15.2)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.21.0->trl==0.15.2)\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.21.0->trl==0.15.2)\n",
      "  Downloading frozenlist-1.7.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.21.0->trl==0.15.2)\n",
      "  Downloading multidict-6.4.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.21.0->trl==0.15.2)\n",
      "  Downloading propcache-0.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.21.0->trl==0.15.2)\n",
      "  Downloading yarl-1.20.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.9/73.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /venv/main/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.21.0->trl==0.15.2) (1.17.0)\n",
      "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.14.0-py3-none-any.whl (374 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.8/374.8 kB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.15.2-py3-none-any.whl (318 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.7.0-py3-none-any.whl (362 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.1/362.1 kB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.3.0-cp312-cp312-manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.7.1-cp312-cp312-manylinux_2_28_x86_64.whl (821.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.0/821.0 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m100.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.3.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.2/243.2 kB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.7/146.7 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-20.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (42.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m93.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m93.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m91.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.12.12-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading frozenlist-1.7.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.8/241.8 kB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.4.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.8/223.8 kB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading propcache-0.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (224 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.4/224.4 kB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.20.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (355 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m355.6/355.6 kB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytz, nvidia-cusparselt-cu12, mpmath, xxhash, tzdata, sympy, setuptools, safetensors, regex, pyarrow, propcache, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, multidict, mdurl, MarkupSafe, fsspec, frozenlist, dill, attrs, aiohappyeyeballs, yarl, triton, pandas, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, multiprocess, markdown-it-py, jinja2, aiosignal, tokenizers, rich, nvidia-cusolver-cu12, aiohttp, transformers, torch, datasets, accelerate, trl, peft\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "Successfully installed MarkupSafe-3.0.2 accelerate-1.7.0 aiohappyeyeballs-2.6.1 aiohttp-3.12.12 aiosignal-1.3.2 attrs-25.3.0 datasets-3.6.0 dill-0.3.8 frozenlist-1.7.0 fsspec-2025.3.0 jinja2-3.1.6 markdown-it-py-3.0.0 mdurl-0.1.2 mpmath-1.3.0 multidict-6.4.4 multiprocess-0.70.16 networkx-3.5 numpy-2.3.0 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 pandas-2.3.0 peft-0.14.0 propcache-0.3.2 pyarrow-20.0.0 pytz-2025.2 regex-2024.11.6 rich-14.0.0 safetensors-0.5.3 setuptools-80.9.0 sympy-1.14.0 tokenizers-0.21.1 torch-2.7.1 transformers-4.51.3 triton-3.3.1 trl-0.15.2 tzdata-2025.2 xxhash-3.5.0 yarl-1.20.1\n",
      "Collecting bitsandbytes==0.45.3\n",
      "  Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting datasets==3.3.2\n",
      "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting accelerate==1.4.0\n",
      "  Downloading accelerate-1.4.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting evaluate==0.4.3\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: torch<3,>=2.0 in /venv/main/lib/python3.12/site-packages (from bitsandbytes==0.45.3) (2.7.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /venv/main/lib/python3.12/site-packages (from bitsandbytes==0.45.3) (2.3.0)\n",
      "Requirement already satisfied: filelock in /venv/main/lib/python3.12/site-packages (from datasets==3.3.2) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /venv/main/lib/python3.12/site-packages (from datasets==3.3.2) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /venv/main/lib/python3.12/site-packages (from datasets==3.3.2) (0.3.8)\n",
      "Requirement already satisfied: pandas in /venv/main/lib/python3.12/site-packages (from datasets==3.3.2) (2.3.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in /venv/main/lib/python3.12/site-packages (from datasets==3.3.2) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /venv/main/lib/python3.12/site-packages (from datasets==3.3.2) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /venv/main/lib/python3.12/site-packages (from datasets==3.3.2) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /venv/main/lib/python3.12/site-packages (from datasets==3.3.2) (0.70.16)\n",
      "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets==3.3.2)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /venv/main/lib/python3.12/site-packages (from datasets==3.3.2) (3.12.12)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /venv/main/lib/python3.12/site-packages (from datasets==3.3.2) (0.30.2)\n",
      "Requirement already satisfied: packaging in /venv/main/lib/python3.12/site-packages (from datasets==3.3.2) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /venv/main/lib/python3.12/site-packages (from datasets==3.3.2) (6.0.2)\n",
      "Requirement already satisfied: psutil in /venv/main/lib/python3.12/site-packages (from accelerate==1.4.0) (7.0.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /venv/main/lib/python3.12/site-packages (from accelerate==1.4.0) (0.5.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /venv/main/lib/python3.12/site-packages (from aiohttp->datasets==3.3.2) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /venv/main/lib/python3.12/site-packages (from aiohttp->datasets==3.3.2) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /venv/main/lib/python3.12/site-packages (from aiohttp->datasets==3.3.2) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /venv/main/lib/python3.12/site-packages (from aiohttp->datasets==3.3.2) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /venv/main/lib/python3.12/site-packages (from aiohttp->datasets==3.3.2) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /venv/main/lib/python3.12/site-packages (from aiohttp->datasets==3.3.2) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /venv/main/lib/python3.12/site-packages (from aiohttp->datasets==3.3.2) (1.20.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /venv/main/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets==3.3.2) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /venv/main/lib/python3.12/site-packages (from requests>=2.32.2->datasets==3.3.2) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /venv/main/lib/python3.12/site-packages (from requests>=2.32.2->datasets==3.3.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /venv/main/lib/python3.12/site-packages (from requests>=2.32.2->datasets==3.3.2) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /venv/main/lib/python3.12/site-packages (from requests>=2.32.2->datasets==3.3.2) (2025.4.26)\n",
      "Requirement already satisfied: setuptools in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.3) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.3) (1.14.0)\n",
      "Requirement already satisfied: networkx in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.3) (3.5)\n",
      "Requirement already satisfied: jinja2 in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.3) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.3) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.3) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.3) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.3) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.3) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.3) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.3) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.3) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.3) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.3) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.3) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.3) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.3) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.3) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /venv/main/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes==0.45.3) (3.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /venv/main/lib/python3.12/site-packages (from pandas->datasets==3.3.2) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /venv/main/lib/python3.12/site-packages (from pandas->datasets==3.3.2) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /venv/main/lib/python3.12/site-packages (from pandas->datasets==3.3.2) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /venv/main/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets==3.3.2) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /venv/main/lib/python3.12/site-packages (from sympy>=1.13.3->torch<3,>=2.0->bitsandbytes==0.45.3) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /venv/main/lib/python3.12/site-packages (from jinja2->torch<3,>=2.0->bitsandbytes==0.45.3) (3.0.2)\n",
      "Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.4.0-py3-none-any.whl (342 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m342.1/342.1 kB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fsspec, datasets, bitsandbytes, accelerate, evaluate\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.0\n",
      "    Uninstalling fsspec-2025.3.0:\n",
      "      Successfully uninstalled fsspec-2025.3.0\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 3.6.0\n",
      "    Uninstalling datasets-3.6.0:\n",
      "      Successfully uninstalled datasets-3.6.0\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 1.7.0\n",
      "    Uninstalling accelerate-1.7.0:\n",
      "      Successfully uninstalled accelerate-1.7.0\n",
      "Successfully installed accelerate-1.4.0 bitsandbytes-0.45.3 datasets-3.3.2 evaluate-0.4.3 fsspec-2024.12.0\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting protobuf\n",
      "  Downloading protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Collecting tensorboard\n",
      "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting absl-py>=0.4 (from tensorboard)\n",
      "  Downloading absl_py-2.3.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard)\n",
      "  Downloading grpcio-1.73.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard)\n",
      "  Downloading markdown-3.8-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /venv/main/lib/python3.12/site-packages (from tensorboard) (2.3.0)\n",
      "Requirement already satisfied: packaging in /venv/main/lib/python3.12/site-packages (from tensorboard) (25.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /venv/main/lib/python3.12/site-packages (from tensorboard) (80.9.0)\n",
      "Requirement already satisfied: six>1.9 in /venv/main/lib/python3.12/site-packages (from tensorboard) (1.17.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /venv/main/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
      "Downloading sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl (321 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.1/321.1 kB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.3.0-py3-none-any.whl (135 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.7/135.7 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.73.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading markdown-3.8-py3-none-any.whl (106 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.2/106.2 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece, werkzeug, tensorboard-data-server, protobuf, markdown, grpcio, absl-py, tensorboard\n",
      "Successfully installed absl-py-2.3.0 grpcio-1.73.0 markdown-3.8 protobuf-6.31.1 sentencepiece-0.2.0 tensorboard-2.19.0 tensorboard-data-server-0.7.2 werkzeug-3.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.51.3 peft==0.14.0 trl==0.15.2\n",
    "\n",
    "# (4) 양자화 및 데이터셋 처리 라이브러리 설치\n",
    "!pip install bitsandbytes==0.45.3 datasets==3.3.2 accelerate==1.4.0 evaluate==0.4.3\n",
    "\n",
    "# (5) 기타 필요한 패키지 설치\n",
    "!pip install sentencepiece protobuf tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77da50c-79d9-41da-a70f-ddd282b51353",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1차 학습코드\n",
    "\n",
    "1차 학습코드. train test split -> 후에 Jsonl Map 해서 학습시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4accc046-a561-4b71-aa2c-fae3e614cea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c2e18d09f9e4c8ba0584ca911aa696d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee74fc4158254f4eac246490ea7ba93d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# JSONL 파일을 Dataset으로 불러오기\n",
    "dataset = load_dataset(\"json\", data_files=\"blog_sft_data.jsonl\", split=\"train\")\n",
    "\n",
    "# 'prompt'와 'completion'을 대화 형식의 messages로 변환하는 함수 정의\n",
    "def to_conversation(sample):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": sample[\"prompt\"]},       # 사용자 프롬프트\n",
    "            {\"role\": \"assistant\", \"content\": sample[\"completion\"]}  # 모델이 생성해야 할 답변\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# 데이터셋의 각 샘플을 변환하고, 필요 없는 원본 열(drop) 제거\n",
    "dataset = dataset.map(to_conversation, remove_columns=dataset.column_names)\n",
    "#  dataset.train_test_split(test_size=0.1)\n",
    "train_data = data_split[\"train\"]\n",
    "eval_data  = data_split[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4b211ec-cc69-404f-8f47-94d4babf7252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90d289e11cd14deca303c81a5270805a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "944e4c11a113472187988da65e96259f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/90.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbd8b838b1384f279967e2757732035a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2f4041f7a0544088119359e5e9f65b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e6fbee657e04773b2df057b01bf6674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.64G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da1f53efec8f4da1ad9c8e15ff8261a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f40150e820954e2f862e6fce1d55074a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f87bd9c8955a420ab5075f6bd112228f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aa15237b7274343b02fc457be618ba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eca0df6635b41b5ad678ac1b23fc9d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c97936c0188e449e949f60a6b1c3c2fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a234fb9f59c448f084a4ad0b3b300afa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1224/720892695.py:71: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = SFTTrainer(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a3d3422083e49039709e8f7cc18e97b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting train dataset to ChatML:   0%|          | 0/270 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fda1c7310ad1484ba7eb67409fbbddac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/270 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41cc8c7dd0c64449a429f4b604f16131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/270 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "480e2a9043864969af4763c35fe69d8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/270 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60d2e65abc8348ed8e8f76d3a1bb7128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting eval dataset to ChatML:   0%|          | 0/30 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9ba1ad751e947b3a4f0f1b3cd776518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to eval dataset:   0%|          | 0/30 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "879ac394f7a545a5a866130fb013cebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/30 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "514ebff079d04e73b86db1fbdc1f09b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/30 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='201' max='201' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [201/201 07:50, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>13.493100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>9.349400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>8.393000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>8.051400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>8.011200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>7.518700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>7.194700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>7.074200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>6.966900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>7.170700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>6.909000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>6.841200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>7.044800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>6.127200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>5.992500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>5.493400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>5.855900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>5.835600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>5.795300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>5.848500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/venv/main/lib/python3.12/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/venv/main/lib/python3.12/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/venv/main/lib/python3.12/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForImageTextToText, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# 1) 모델과 토크나이저 로드 (4-bit 양자화 설정)\n",
    "model_id = \"google/gemma-3-4b-it\"  # 파인튜닝할 Gemma 3 4B Instruction Tuned 모델\n",
    "\n",
    "\n",
    "# torch랑 bfloat16 설정 (최신 글카면 bfloat16 사용 가능합니다)\n",
    "if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8:\n",
    "    torch_dtype = torch.bfloat16\n",
    "else:\n",
    "    torch_dtype = torch.float16\n",
    "\n",
    "# BitsAndBytes 4-bit 양자화 설정 생성:contentReference[oaicite:16]{index=16}\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,      # 2중 양자화 사용 (메모리 효율 향상)\n",
    "    bnb_4bit_quant_type=\"nf4\",           # NF4 양자화 타입\n",
    "    bnb_4bit_compute_dtype=torch_dtype   # bfloat 16 사용\n",
    ")\n",
    "\n",
    "# 모델 로드\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"eager\",   # FlashAttention 미사용 => 이상하게 버그발생\n",
    "    torch_dtype=torch_dtype\n",
    ")\n",
    "\n",
    "# 토크나이저\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# 2) LoRA 어댑터 구성 설정:contentReference[oaicite:18]{index=18}\n",
    "peft_config = LoraConfig(\n",
    "    r=16,              # Rank \n",
    "    lora_alpha=16,     # LoRA scaling\n",
    "    lora_dropout=0.05, # 드롭아웃 비율 (QLoRA 논문 권장 0.05)\n",
    "    bias=\"none\",       # Bias는\n",
    "    target_modules=\"all-linear\",  # 모든 Linear 계층에 LoRA 적용\n",
    "    task_type=\"CAUSAL_LM\"         # 언어모델 과제 타입\n",
    ")\n",
    "# (참고) Gemma 모델의 경우 embedding과 LM 헤드 등 일부 모듈은 LoRA 적용 대상에서 제외하거나 별도 저장 필요\n",
    "\n",
    "# 3) 파인튜닝 하이퍼파라미터 설정 (SFTConfig 사용)\n",
    "train_args = SFTConfig(\n",
    "    output_dir=\"gemma3-4b-blog-qlora\",   # 출력 디렉토리 (및 HF Hub 리포지토리 이름)\n",
    "    max_seq_length=512,                 # 최대 시퀀스 길이 (512 토큰)\n",
    "    packing=False,                      # (여기서는 packing 미사용; 필요시 True로 설정 가능)\n",
    "    num_train_epochs=3,                 # 에포크 수 (예: 3)\n",
    "    per_device_train_batch_size=1,      # GPU 한 장당 배치 사이즈\n",
    "    gradient_accumulation_steps=4,      # 그래디언트 누적 스텝 \n",
    "    gradient_checkpointing=True,        # 그래디언트 체크포인팅 활성화 (메모리 절약):\n",
    "    optim=\"adamw_torch_fused\",          # PyTorch fused AdamW 옵티마이저 사용:\n",
    "    learning_rate=2e-4,                 # 학습률 \n",
    "    max_grad_norm=0.3,                  # 그래디언트 클리핑 임계값 \n",
    "    warmup_ratio=0.03,                  # 워밍업 비율\n",
    "    lr_scheduler_type=\"constant\",       # ???\n",
    "    fp16=True if torch_dtype == torch.float16 else False,   \n",
    "    bf16=True if torch_dtype == torch.bfloat16 else False,  \n",
    "    logging_steps=10,                   # 로깅 간격 \n",
    "    save_strategy=\"epoch\",              # 체크포인트 저장 간격 (에포크마다 저장)\n",
    "    push_to_hub=False,                 \n",
    "    report_to=\"tensorboard\"            \n",
    "\n",
    ")\n",
    "\n",
    "# 4) SFTTrainer 초기화 및 훈련 시작\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=train_data,      # 앞서 준비한 학습 Dataset\n",
    "    eval_dataset=eval_data,        # (옵션) 검증 Dataset\n",
    "    peft_config=peft_config,\n",
    "    # 데이터 처리용 토크나이저/전처리 클래스 지정 \n",
    "    tokenizer=tokenizer  \n",
    ")\n",
    "\n",
    "# 모델 훈련 실행 (학습 진행 상황은 로그로 출력됨)\n",
    "trainer.train()\n",
    "\n",
    "# 5) 학습 완료 후 어댑터 모델 저장\n",
    "trainer.save_model()  # LoRA 어댑터 가중치를 output_dir에 저장\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d137bd-d882-4bb1-b38c-3a3f8dd6c64f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Qlora 추가학습 코드\n",
    "1차 실험에서 이어나가는 코드 (Json 다른거 이름만 같게하면 무조건 추가학습되니 한번 짜고 계속 써먹을것)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a9e3b3-e622-415c-98e1-9920775fcdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import load_dataset\n",
    "\n",
    "# === 0. Dataset 불러오기 및 전처리 ===\n",
    "dataset = load_dataset(\"json\", data_files=\"blog_sft_data.jsonl\", split=\"train\")\n",
    "\n",
    "def to_conversation(sample):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": sample[\"prompt\"]},\n",
    "            {\"role\": \"assistant\", \"content\": sample[\"completion\"]}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(to_conversation, remove_columns=dataset.column_names)\n",
    "data_split = dataset.train_test_split(test_size=0.1)\n",
    "train_data = data_split[\"train\"]\n",
    "eval_data  = data_split[\"test\"]\n",
    "\n",
    "# === 1. 모델 로드 및 4bit 양자화 설정 ===\n",
    "model_id = \"google/gemma-3-4b-it\"\n",
    "\n",
    "if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8:\n",
    "    torch_dtype = torch.bfloat16\n",
    "else:\n",
    "    torch_dtype = torch.float16\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype\n",
    ")\n",
    "\n",
    "# 기본 모델 로드\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"eager\",\n",
    "    torch_dtype=torch_dtype\n",
    ")\n",
    "\n",
    "# 이미 학습된 LoRA 어댑터 불러오기 (.safetensors 기반 이어받기)\n",
    "model = PeftModel.from_pretrained(base_model, \"gemma3-4b-blog-qlora\")\n",
    "\n",
    "# === 2. 토크나이저 로드 ===\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# === 3. LoRA 설정 ===\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=\"all-linear\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# === 4. SFT 학습 설정 ===\n",
    "train_args = SFTConfig(\n",
    "    output_dir=\"gemma3-4b-blog-qlora\",   # 같은 디렉토리에 덮어쓰기됨\n",
    "    max_seq_length=512,\n",
    "    packing=False,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    learning_rate=2e-4,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    fp16=(torch_dtype == torch.float16),\n",
    "    bf16=(torch_dtype == torch.bfloat16),\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    push_to_hub=False,\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "# === 5. Trainer 생성 및 훈련 ===\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=eval_data,\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# === 6. 학습된 LoRA 어댑터 저장 ===\n",
    "trainer.save_model()  # gemma3-4b-blog-qlora/ 에 adapter_model.safetensors 저장\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd97509-0590-4944-a4bb-a7312b226148",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# QLora 로딩\n",
    "로딩, Lora 층을 model과 합체하는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a0fc77e-105e-4060-9792-ad1baa52c765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bc72966b4b346e6a0b065d5691531b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# FlashAttention 및 SDPA 비활성화 (alignment 문제 방지용)\n",
    "os.environ[\"FLASH_ATTENTION_FORCE_DISABLE\"] = \"1\"\n",
    "os.environ[\"SDPA_FORCE_DISABLE\"] = \"1\"\n",
    "os.environ[\"USE_FLASH_ATTENTION\"] = \"0\"\n",
    "os.environ[\"USE_MEM_EFFICIENT_ATTENTION\"] = \"0\"\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# 1. 기본(base) 모델 로드\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-3-4b-it\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"eager\"  # attention flash 미사용 !!!!!이거 안끄면 버그남 !!!!!!!!\n",
    ")\n",
    "\n",
    "# 2. 어댑터 적용\n",
    "model = PeftModel.from_pretrained(base_model, \"gemma3-4b-blog-qlora\")  # 여기에 adapter_model.safetensors 있음\n",
    "\n",
    "# 3. 토크나이저도 함께 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gemma3-4b-blog-qlora\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b5e717-e011-48d5-88e6-fd66726372e3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Qlora 추론코드\n",
    "프롬프트와 추론하는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbda0c11-390b-40ad-8e55-de3691e96b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "당신은 블로그를 포스팅하는 블로거입니다. 카페에 대한 글을 쓸 예정입니다.[가게이름:연남동 라이프카페] [맛:좀 짠데 맛있음] [경치:주차장부터멋있음] [가격:비쌈] [분위기:넓음빈티지조용함] 의 내용으로 블로그를 포스팅해주세요. 이모지(👍💕..)나 특수기호($*#@)는 사용하지 마세요. 최대한 길게 쓰세요. 최대한 사람처럼 쓰세요.[추가정보:용산구 연남고 2길 양제아파트 2층, 주차장부터 멋있고 경치가 서울 한복판이 보여 좋고, 고급진 닭갈비가 맛있었다고 해야함]\n",
      "예쁜 카페 찾으러 다녀다니는 편이라서 이번에는 연남동에 있는 카페 중에서도 가장 높은 층에 위치해 있는 연남동 카페였는데 사진에서 보는 것보다 훨씬 더 넓었고 주차장이 있어서 주차가 편했고 야외석도 있어서 날씨 좋은 날에는 야외석에서 커피 한 잔 하는 것도 추천하고 싶어요 그리고 이곳의 특징인 건 바로 바깥쪽에 주차장을 만들었는데 그 주차장이 멋지고 특히 건물 뒤쪽으로 보이는 서울 한복판의 풍경이 정말 예뻤어요\n",
      "한번은 용산구에서 연남동까지 가다가 양제아파트라고 써져있는 것을 보고 놀랐는데 그건 바로 연남동에 있는 카페였어요 그리고 카페 앞에도 작은 주차장이 있었는데 저는 처음에는 그냥 지나칠뻔 했어요 사실 카페 앞으로 주차장이 있는 곳은 제가 본 적 없어서 어떻게 이런 카페가 이곳에 있는지 궁금했어요 그리고 무엇보다도 카페 앞 주차장은 굉장히 멋진데요 용산에서 올라와서 올라오면 바로 보이는 풍경인데 카페 앞에 있는 주차장은 일반적인 주차장이 아니라 고급스러운 주차장이더라고요\n",
      "연남동 카페는 카페 이름은 연남동 라이프카페이고 내부가 매우 넓고 자리도 많았어요 제가 방문했을 때는 평일이었는데 손님들이 많이 오지 않았고 그래서인지 모든 자리가 자유롭게 앉고 움직일 수 있는 공간이었어요\n",
      "그리고 여기는 커피뿐만 아니라 다양한 음료를 판매하고 있었는데 저는 여기서 시그니처 메뉴인 아이스 카푸치노를 주문했어요 그리고 저는 매운 김치볶음밥과 닭갈비를 먹었는데 닭갈비가 고급스러워 보였다는 말로는 닭갈비 치고 가격이 비싸다는 말을 들었지만 정말 맛있었어요 그리고 마늘빵도 맛있는지 여쭤봤더니 직원분이 네 맞아요 마늘빵도 정말 맛있다 하셨어요\n",
      "그리고 음료도 너무너무 맛있었고 디저트도 다양해서 어떤 디저트를 고를지 고민할 때마다 행복했어요 그리고 무엇보다도 이곳의 장점은 바로 창밖 풍경이었어요 창밖으로 보이는 서울 한복판의 풍경이 정말 아름다웠는데 특히 해가 질 때쯤에는 감성적인 분위기가 느껴졌어요\n",
      "저는 이곳에서 시간을 보내면서 사진도 많이 찍었고 주변 풍경도 담았는데 카메라를 너무 오래 들고 다니지 않아도 되어서 그런지 오히려 자연스럽게 풍경을 즐기고 사진을 남길 수 있었던 것 같아요\n",
      "그리고 이곳은 반려동물 동반도 가능하더라고요\n",
      "서울에서 주차장에 있는 카페라기보다는 고급스러운 느낌의 카페였고 주차장도 고급스러워서 그런지 오히려 용산에서 올라와서 이곳에 도착하면 뭔가 다른 도시로 온 듯한 느낌이 들었어요 그리고 무엇보다도 이곳에서 보내는 시간이 즐거운 시간이었답니다\n",
      "주말이나 공휴일에 가면 손님이 많을 수도 있으니 참고하세요\n",
      "그리고 주차장에 있는 푸른 새집도 예쁘죠\n",
      "내돈내산 후기입니다\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# 모델과 토크나이저는 이미 아래처럼 로딩된 상태라고 가정\n",
    "# model = PeftModel.from_pretrained(base_model, \"gemma3-4b-blog-qlora\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"gemma3-4b-blog-qlora\")\n",
    "\n",
    "prompt = \"당신은 블로그를 포스팅하는 블로거입니다. 카페에 대한 글을 쓸 예정입니다.[가게이름:연남동 라이프카페] [맛:맛있음] [경치:주차장부터멋있음] [가격:비쌈] [분위기:넓음빈티지조용함] 의 내용으로 블로그를 포스팅해주세요. 이모지(👍💕..)나 특수기호($*#@)는 사용하지 마세요. 최대한 길게 쓰세요. 최대한 사람처럼 쓰세요.\"\n",
    "\n",
    "# 토크나이즈\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# 텍스트 생성\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=1000,         # 생성할 최대 토큰 수\n",
    "        do_sample=True,             # sampling 방식 (True면 다양성 ↑)\n",
    "        temperature=0.7,            # 샘플링 온도 (낮을수록 덜 창의적)\n",
    "        top_p=0.9,                  # nucleus sampling\n",
    "        repetition_penalty=1.1      # 반복 방지\n",
    "    )\n",
    "\n",
    "# 디코딩 및 출력\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3be86a7",
   "metadata": {},
   "source": [
    "### end of token 설정 다시할것!! (코드 있는데 날라감. Fast api 사용할때 수정 사용)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab69b548-7604-42a9-809b-0c78afc6f715",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Qlora Auto Complete 기능\n",
    "실험단계 todo : 기능개선"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aea9f8a1-b333-43cd-8cb4-a193d2611be4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Generated Blog Post ===\n",
      "안녕하세요\n",
      "저는 매일 아침 6시반에 일어나는 버릇없는 사람입니다\n",
      "그래서 저는 보통 오전 8시쯤에 일어나서 간단히 아침을 먹고\n",
      "카페로 향합니다\n",
      "오늘도 그런 날이었어요\n",
      "아침 식사는 집에서 간단히 해결하고\n",
      "카페로 향했습니다\n",
      "그리고 바로 옆에 있는 카페 Anuk으로 갔습니다\n",
      "이곳은 제가 가장 좋아하는 카페 중 하나인데요\n",
      "그 이유는 무엇일까요\n",
      "첫째는 이곳의 뷰가 너무 좋습니다\n",
      "두 번째는 이곳의 커피 맛이 좋다는 것입니다\n",
      "세 번째는 이곳의 분위기가 너무 좋아요\n",
      "마지막으로 이곳의 디저트가 맛있다는 점입니다\n",
      "Anuk은 신촌역 4번 출구에서 도보 5분 거리에 있습니다\n",
      "카페 Anuk은 빨간 벽돌 건물 2층에 위치해 있어요\n",
      "카페 Anuk은 주차 지원은 안되지만 대중교통이나 자전거로 오시는 분들은 편리하게 이용하실 수 있습니다\n",
      "카페 Anuk은 음료와 디저트 메뉴가 다양해서\n",
      "취향대로 골라 드실 수 있답니다\n",
      "음료는 아이스커피 레몬티 초콜릿라떼 녹차티라미수케이크\n",
      "디저트는 초코칩쿠키 바스크치즈케이크 시나모롤 등이 있어요\n",
      "저는 오늘 날씨가 흐리고 비가 와서 그런지\n",
      "초콜릿라떼가 더 맛있게 느껴졌는데요\n",
      "초콜릿라떼는 기본 라떼에 초콜릿 시럽과 휘핑크림이 올라가는 메뉴예요\n",
      "저는 초콜릿 시럽을 조금만 넣어달라고 했고\n",
      "휘핑크림은 빼달라고 부탁드렸습니다\n",
      "그럼에도 불구하고 휘핑크림이 올라가 있었는데\n",
      "원래는 휘핑크림을 빼달라고 하지 않으셨어야 했는데\n",
      "너무 아쉬웠어요\n",
      "이런 건 주문할 때 미리 말씀드려야 하는 게 아닌가 싶네요\n",
      "초콜릿 시럽은 정말 많이 넣으셔서\n",
      "라떼가 단맛이 강했어요\n",
      "다음에는 초콜릿 시럽을 조금만 넣어달라고 하거나\n",
      "초콜릿 시럽을 넣지 않고 라떼만 주문해야겠어요\n",
      "그리고 휘핑크림은 꼭 빼달라고 말씀드려야 합니다\n",
      "초콜릿라떼가 너무 달아서\n",
      "얼마 먹지 못하고 남겨버렸답니다\n",
      "바스크치즈케이크는 정말 맛있었는데요\n",
      "크림이 느끼하지 않고 바삭한 크러스트 위에 올려져 있어서\n",
      "진짜 맛있었어요\n",
      "제가 또 치즈케이크를 좋아하는데\n",
      "여기 치즈케이크는 진짜 최고였어요\n",
      "저는 매장에서 사 왔는데\n",
      "지금은 온라인에서도 구매 가능하더라고요\n",
      "온라인에서도 구매 가능하니 참고하시고요\n",
      "Anuk은 특히 저녁 시간에 운영을 하는데\n",
      "저는 평일에는 주로 아침이나 점심에 방문을 하는 편이에요\n",
      "주말에는 오후에 방문을 하곤 하는데요\n",
      "평일에 방문하기에는 좋은 시간대가 아닌가 싶기도 합니다\n",
      "그렇다고 해서 주말에 방문하는 것이 나쁘지는 않아요\n",
      "오후에 커피 한 잔 하고 가기에는 괜찮은데\n",
      "저는 평일에는 좀 더 여유 있게 시간을 보내는 것을 좋아하기 때문에\n",
      "주말에 방문을 하는 것 같아요\n",
      "An\n"
     ]
    }
   ],
   "source": [
    "# 모델 및 토크나이저 로딩은 완료되었다고 가정합니다.\n",
    "\n",
    "def generate_blog_post(cafe_details):\n",
    "    \"\"\"\n",
    "    안정적인 블로그 포스트 생성을 위한 함수\n",
    "    \"\"\"\n",
    "    # 1. 일관성을 위해 명확한 프롬프트 템플릿 사용\n",
    "    prompt_template = \"당신은 블로그를 포스팅하는 블로거입니다. {category}에 대한 글을 쓸 예정입니다.[가게이름:{cafe_name}] [맛:{taste}] [경치:{view}] [가격:{price}] [분위기:{atmosphere}] 의 내용으로 블로그를 포스팅해주세요. 이모지(👍💕..)나 특수기호($*#@)는 사용하지 마세요. 최대한 길게 쓰세요. 최대한 사람처럼 쓰세요.\"\n",
    "    \n",
    "    # 2. 템플릿에 맞춰 깨끗한 프롬프트 생성\n",
    "    prompt = prompt_template.format(**cafe_details)\n",
    "\n",
    "    # 항상 시작 토큰(bos_token)을 맨 앞에 추가\n",
    "    if tokenizer.bos_token:\n",
    "        prompt = tokenizer.bos_token + prompt\n",
    "\n",
    "    # 깨끗하게 준비된 프롬프트를 토크나이징\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # 종료 토큰 리스트 정의\n",
    "    terminators = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<end_of_turn>\")\n",
    "    ]\n",
    "\n",
    "    # 텍스트 생성\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1000,\n",
    "            eos_token_id=terminators,\n",
    "            do_sample=True,\n",
    "            temperature=0.65,  # 안정성을 위해 온도를 살짝 낮춤\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1\n",
    "        )\n",
    "\n",
    "    # 생성된 텍스트 디코딩 및 후처리\n",
    "    # 입력 프롬프트 부분을 제외하고, <end_of_turn> 앞에서 잘라내기\n",
    "    generated_text = tokenizer.decode(outputs[0][len(inputs['input_ids'][0]):], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text.strip()\n",
    "\n",
    "# 예시\n",
    "gwangju_cafe_details = {\n",
    "    \"category\" : \"카페\",\n",
    "    \"cafe_name\": \"Anuk .카페\",\n",
    "    \"taste\": \"라떼가 달콤함\",\n",
    "    \"view\": \"뷰\",\n",
    "    \"price\": \"정보없음\",\n",
    "    \"atmosphere\": \"아늑함\"\n",
    "}\n",
    "\n",
    "# 함수를 호출하여 블로그 포스트 생성\n",
    "blog_post = generate_blog_post(gwangju_cafe_details)\n",
    "print(\"=== Generated Blog Post ===\")\n",
    "print(blog_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "046dc0f5-5a35-4630-ba57-df8adb45816e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocomplete(prompt: str, max_new_tokens: int = 64):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,          # 확정적 (greedy) 자동완성\n",
    "            temperature=0.0,\n",
    "            repetition_penalty=1.05,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # 전체 결과 디코딩\n",
    "    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # 원래 prompt 이후 텍스트 추출\n",
    "    completion = full_text[len(prompt):]\n",
    "\n",
    "    # 줄바꿈(\\n) 이전까지만 자르기\n",
    "    return completion.split(\"\\n\")[0].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7dcf4a9a-040e-4bbd-89c4-82eefecca5bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'훌륭한 카페에 다녀왔습니다.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autocomplete(\"2층 테라스 분위기가 \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2cfa7b-0bed-45f4-bcb7-e32a5c40a7de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
