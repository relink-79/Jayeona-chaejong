from fastapi import FastAPI, Request, Form
from fastapi.responses import HTMLResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel
from typing import Optional
import json
import traceback
from pydantic import BaseModel

app = FastAPI(title="ë¸”ë¡œê·¸ í¬ìŠ¤íŒ… ìë™ìƒì„±ê¸°")

# ì •ì  íŒŒì¼ê³¼ í…œí”Œë¦¿ ì„¤ì •
app.mount("/static", StaticFiles(directory="static"), name="static")
templates = Jinja2Templates(directory="templates")

class BlogGenerator:
    def __init__(self, base_model_name: str, adapter_path: str):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"Loading base model '{base_model_name}' on {self.device}...")

        # ê³„ì‚° ë°ì´í„° íƒ€ì…ì„ bfloat16ìœ¼ë¡œ í†µì¼í•˜ì—¬ ì•ˆì •ì„± í™•ë³´
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=True,
        )

        # ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            quantization_config=bnb_config,
            device_map="auto",
            torch_dtype=torch.bfloat16 # ëª¨ë¸ ê¸°ë³¸ íƒ€ì…ë„ ëª…ì‹œì ìœ¼ë¡œ bfloat16 ì‚¬ìš©
        )
        
        # í† í¬ë‚˜ì´ì €ë¥¼ ì–´ëŒ‘í„° ê²½ë¡œì—ì„œ ë¡œë“œí•´ì•¼ í•©ë‹ˆë‹¤.
        self.tokenizer = AutoTokenizer.from_pretrained(adapter_path)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            base_model.config.pad_token_id = base_model.config.eos_token_id

        print(f"Loading QLoRA adapter from '{adapter_path}'...")
        self.model = PeftModel.from_pretrained(base_model, adapter_path)
        self.model.eval()
        print("Model and adapter loaded successfully!")

    def generate_blog_post(self, category: str, fields: dict, details: str) -> str:
        """ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒì„± (ipynbì™€ í”„ë¡¬í”„íŠ¸ í˜•ì‹ í†µì¼)"""
        
        # 1. ipynbì™€ ë™ì¼í•œ í”„ë¡¬í”„íŠ¸ í˜•ì‹ì„ ìœ„í•œ í•„ë“œ ë¬¸ìì—´ ìƒì„±
        key_map = {
            'store_name': 'ê°€ê²Œì´ë¦„', 'taste': 'ë§›', 'view': 'ê²½ì¹˜', 
            'price': 'ê°€ê²©', 'atmosphere': 'ë¶„ìœ„ê¸°', 'food_type': 'ìŒì‹ì¢…ë¥˜',
            'rating': 'í‰ê°€', 'product_name': 'ì œí’ˆì´ë¦„', 'category': 'ì¢…ë¥˜',
            'purpose': 'ìš©ë„'
        }
        
        field_parts = []
        # 'undefined' ë¬¸ìì—´ì„ ê±¸ëŸ¬ë‚´ê¸° ìœ„í•œ ì¡°ê±´ ì¶”ê°€
        for key, value in fields.items():
            # ê°’ì´ ì¡´ì¬í•˜ê³ , 'undefined' ë¬¸ìì—´ì´ ì•„ë‹ ë•Œë§Œ ì¶”ê°€
            if value and str(value).strip().lower() != 'undefined':
                kor_key = key_map.get(key, key)
                field_parts.append(f"[{kor_key}:{value}]")

        # ìƒì„¸ë‚´ìš©ì€ ê°’ì´ ìˆê³ , 'undefined'ê°€ ì•„ë‹ ë•Œë§Œ (ê³µë°±ë§Œ ìˆëŠ” ê²½ìš° ì œì™¸) í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€
        if details and details.strip() and details.strip().lower() != 'undefined':
            field_parts.append(f"[ìƒì„¸ë‚´ìš©:{details}]")
        
        field_str = "".join(field_parts)

        # 2. ì¹´í…Œê³ ë¦¬ë³„ ì£¼ì œ ë¬¸ì¥ ì„¤ì •
        subject_map = {
            "ì¹´í˜": "ì¹´í˜ì— ëŒ€í•œ ê¸€ì„ ì“¸ ì˜ˆì •ì…ë‹ˆë‹¤.",
            "ë§›ì§‘": "ë§›ì§‘ì— ëŒ€í•œ ê¸€ì„ ì“¸ ì˜ˆì •ì…ë‹ˆë‹¤.",
            "ë¦¬ë·°": "ì œí’ˆ ë¦¬ë·°ì— ëŒ€í•œ ê¸€ì„ ì“¸ ì˜ˆì •ì…ë‹ˆë‹¤."
        }
        subject = subject_map.get(category, f"{category}ì— ëŒ€í•œ ê¸€ì„ ì“¸ ì˜ˆì •ì…ë‹ˆë‹¤.")
        
        # 3. ìµœì¢… í”„ë¡¬í”„íŠ¸ ì¡°í•© (ipynbì™€ ì™„ì „ ë™ì¼)
        prompt = (
            f"ë‹¹ì‹ ì€ ë¸”ë¡œê·¸ë¥¼ í¬ìŠ¤íŒ…í•˜ëŠ” ë¸”ë¡œê±°ì…ë‹ˆë‹¤. {subject}"
            f"{field_str} ì˜ ë‚´ìš©ìœ¼ë¡œ ë¸”ë¡œê·¸ë¥¼ í¬ìŠ¤íŒ…í•´ì£¼ì„¸ìš”. "
            "ì´ëª¨ì§€(ğŸ‘ğŸ’•..)ë‚˜ íŠ¹ìˆ˜ê¸°í˜¸($*#@)ëŠ” ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”. "
            "ìµœëŒ€í•œ ê¸¸ê²Œ ì“°ì„¸ìš”. ìµœëŒ€í•œ ì‚¬ëŒì²˜ëŸ¼ ì“°ì„¸ìš”."
        )

        # ë””ë²„ê¹…ì„ ìœ„í•´ ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ ì¶œë ¥
        print("\n" + "="*50)
        print("Generated Prompt for Model:")
        print(prompt)
        print("="*50 + "\n")

        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=1000,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.1,
                eos_token_id=self.tokenizer.eos_token_id,
                pad_token_id=self.tokenizer.eos_token_id # pad_tokenì„ eos_tokenìœ¼ë¡œ ì„¤ì •
            )
        
        # ì…ë ¥ í”„ë¡¬í”„íŠ¸ë¥¼ ì œì™¸í•œ ìƒì„±ëœ í…ìŠ¤íŠ¸ë§Œ ë””ì½”ë”©
        generated_text = self.tokenizer.decode(
            outputs[0],  # ì „ì²´ ì¶œë ¥ì„ ë””ì½”ë”©
            skip_special_tokens=True
        )
        
        # í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ì„ ê²°ê³¼ì—ì„œ ì œê±°
        return generated_text[len(prompt):].strip()

# ì „ì—­ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
blog_generator = None

@app.on_event("startup")
async def startup_event():
    global blog_generator
    base_model_name = "google/gemma-3-4b-it"
    # makeweb/main.py ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œ ìˆ˜ì •
    adapter_path = "../gemma3-4b-blog-qlora" 
    blog_generator = BlogGenerator(base_model_name=base_model_name, adapter_path=adapter_path)

@app.get("/", response_class=HTMLResponse)
async def home(request: Request):
    """ë©”ì¸ í˜ì´ì§€"""
    return templates.TemplateResponse("index.html", {"request": request})

@app.post("/generate")
async def generate_blog(
    request: Request,
    category: str = Form(...),
    details: str = Form(...),
    cafe_taste: Optional[str] = Form(None), cafe_view: Optional[str] = Form(None), cafe_price: Optional[str] = Form(None), cafe_atmosphere: Optional[str] = Form(None), cafe_store_name: Optional[str] = Form(None),
    restaurant_taste: Optional[str] = Form(None), restaurant_food_type: Optional[str] = Form(None), restaurant_rating: Optional[str] = Form(None), restaurant_price: Optional[str] = Form(None), restaurant_store_name: Optional[str] = Form(None),
    review_category: Optional[str] = Form(None), review_rating: Optional[str] = Form(None), review_price: Optional[str] = Form(None), review_purpose: Optional[str] = Form(None), review_product_name: Optional[str] = Form(None)
):
    fields = {}
    if category == "ì¹´í˜":
        fields = {"taste": cafe_taste, "view": cafe_view, "price": cafe_price, "atmosphere": cafe_atmosphere, "store_name": cafe_store_name}
    elif category == "ë§›ì§‘":
        fields = {"taste": restaurant_taste, "food_type": restaurant_food_type, "rating": restaurant_rating, "price": restaurant_price, "store_name": restaurant_store_name}
    else:
        fields = {"category": review_category, "rating": review_rating, "price": review_price, "purpose": review_purpose, "product_name": review_product_name}
    
    try:
        generated_post = blog_generator.generate_blog_post(category, fields, details)
        return {"success": True, "generated_post": generated_post, "category": category, "fields": fields, "details": details}
    except Exception as e:
        traceback.print_exc()
        return {"success": False, "error": str(e)}

class AutocompleteRequest(BaseModel):
    prompt: str

@app.post("/text_autocomplete")
async def text_autocomplete(req: AutocompleteRequest):
    """ì‹¤ì‹œê°„ í…ìŠ¤íŠ¸ ìë™ì™„ì„±ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    try:
        prompt = req.prompt
        inputs = blog_generator.tokenizer(prompt, return_tensors="pt").to(blog_generator.device)

        with torch.no_grad():
            outputs = blog_generator.model.generate(
                **inputs,
                max_new_tokens=20,  # ì§§ê³  ë¹ ë¥¸ ì¶”ì²œì„ ìœ„í•´
                do_sample=False,
                temperature=0.0,
                repetition_penalty=1.05,
                pad_token_id=blog_generator.tokenizer.eos_token_id
            )
        
        full_text = blog_generator.tokenizer.decode(outputs[0], skip_special_tokens=True)
        completion = full_text[len(prompt):]
        # ì¤„ë°”ê¿ˆ ì´ì „ì˜ ì²«ë²ˆì§¸ ë¼ì¸ë§Œ ë°˜í™˜
        suggestion = completion.split("\n")[0].strip()

        return {"success": True, "suggestion": suggestion}
    except Exception as e:
        # traceback.print_exc() # ìƒì„¸ ì˜¤ë¥˜ ë¡œê¹…ì´ í•„ìš”í•  ë•Œ ì£¼ì„ í•´ì œ
        return {"success": False, "error": str(e)}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000) 